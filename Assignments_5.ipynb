{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d788166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a program to estimate the value function for a given policy by iteratively simulating episodes and\n",
    "# updating the value function based on the observed returns.\n",
    "\n",
    "# Name: Sharvari Pramod Jape\n",
    "# Class: B.E AIML\n",
    "# Roll No: 43526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ceb29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value table is:\n",
      "[4.187      3.90650779 4.18755891 4.14327202 3.9241517 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple environment with deterministic transitions\n",
    "# For simplicity, let's assume there are 5 states and \n",
    "# moving from one state to the next gives a reward of 1, with state 4 being terminal\n",
    "\n",
    "class SimpleEnvironment:\n",
    "\tdef __init__(self, num_states=5):\n",
    "\t\tself.num_states = num_states\n",
    "\n",
    "\tdef step(self, state):\n",
    "\t\treward = 0\n",
    "\t\tterminal = False\n",
    "\n",
    "\t\tif state < self.num_states - 1:\n",
    "\t\t\tnext_state = state + 1\n",
    "\t\t\treward = 1\n",
    "\t\telse:\n",
    "\t\t\tnext_state = state\n",
    "\t\t\tterminal = True\n",
    "\n",
    "\t\treturn next_state, reward, terminal\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\treturn 0 # Start from state 0\n",
    "\n",
    "\n",
    "# Define a random policy for the sake of demonstration\n",
    "def random_policy(state, num_actions=5):\n",
    "\treturn np.random.choice(num_actions)\n",
    "\n",
    "\n",
    "# Monte Carlo Policy Evaluation function\n",
    "def monte_carlo_policy_evaluation(policy, env, num_episodes, gamma=1.0):\n",
    "\tvalue_table = np.zeros(env.num_states)\n",
    "\treturns = {state: [] for state in range(env.num_states)}\n",
    "\n",
    "\tfor _ in range(num_episodes):\n",
    "\t\tstate = env.reset()\n",
    "\t\tepisode = []\n",
    "\t\t# Generate an episode\n",
    "\t\twhile True:\n",
    "\t\t\taction = policy(state)\n",
    "\t\t\tnext_state, reward, terminal = env.step(action)\n",
    "\t\t\tepisode.append((state, reward))\n",
    "\t\t\tif terminal:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tstate = next_state\n",
    "\n",
    "\t\t# Calculate the return and update the value table\n",
    "\t\tG = 0\n",
    "\t\tfor state, reward in reversed(episode):\n",
    "\t\t\tG = gamma * G + reward\n",
    "\t\t\treturns[state].append(G)\n",
    "\t\t\tvalue_table[state] = np.mean(returns[state])\n",
    "\n",
    "\treturn value_table\n",
    "\n",
    "\n",
    "# Define the number of episodes for MC evaluation\n",
    "num_episodes = 1000\n",
    "\n",
    "# Create a simple environment instance\n",
    "env = SimpleEnvironment(num_states=5)\n",
    "\n",
    "# Evaluate the policy\n",
    "v = monte_carlo_policy_evaluation(random_policy, env, num_episodes)\n",
    "\n",
    "print(\"The value table is:\")\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4409e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db0028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4cf1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
